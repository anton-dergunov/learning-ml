{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook demonstrates sentiment analysis using LSTM.\n",
    "\n",
    "Original source code: https://github.com/bentrevett/pytorch-sentiment-analysis/blob/master/2%20-%20Upgraded%20Sentiment%20Analysis.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchtext import data\n",
    "from torchtext import datasets\n",
    "import itertools\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: en_core_web_sm==2.0.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.0.0/en_core_web_sm-2.0.0.tar.gz#egg=en_core_web_sm==2.0.0 in /Users/anton/anaconda3/lib/python3.7/site-packages (2.0.0)\n",
      "\n",
      "\u001b[93m    Linking successful\u001b[0m\n",
      "    /Users/anton/anaconda3/lib/python3.7/site-packages/en_core_web_sm -->\n",
      "    /Users/anton/anaconda3/lib/python3.7/site-packages/spacy/data/en\n",
      "\n",
      "    You can now load the model via spacy.load('en')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we use `include_lengths`, this produces a tuple (first element - the text, second element - its length). This allows usage of packed padded sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT = data.Field(tokenize = 'spacy', include_lengths = True)\n",
    "LABEL = data.LabelField(dtype = torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/anton/pytorch_data'"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA_PATH = os.path.expanduser(r\"~/pytorch_data\")\n",
    "DATA_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = datasets.IMDB.splits(TEXT, LABEL, root=DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 25000)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data), len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, valid_data = train_data.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17500, 7500, 25000)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data), len(valid_data), len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_VOCAB_SIZE = 25_000\n",
    "\n",
    "TEXT.build_vocab(train_data,\n",
    "                 max_size = MAX_VOCAB_SIZE,\n",
    "                 vectors = \"glove.6B.100d\",\n",
    "                 vectors_cache = os.path.join(DATA_PATH, \"vector_cache\"))\n",
    "\n",
    "LABEL.build_vocab(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25002"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(TEXT.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([25002, 100])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEXT.vocab.vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0.])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEXT.vocab.vectors[TEXT.vocab.stoi[TEXT.unk_token]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0.])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEXT.vocab.vectors[TEXT.vocab.stoi[TEXT.pad_token]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<unk>, <pad>, the, ,, ., a, and, of, to, is, in, I, it, that, \", \\'s, this, -, /><br, was, as, with, movie, for, film, The, but, on, (, n\\'t, ), you, are, not, have, his, be, he, one, !, at, by, all, who, an, they, from, like, her, so, \\', about, has, or, It, out, just, do, ?, some, good, more, would, very, up, what, This, there, time, can, when, which, she, had, only, story, really, if, were, see, their, even, no, my, did, me, does, than, ..., much, :, get, been, could, into, well, we, will, people, bad'"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "', '.join(itertools.islice(TEXT.vocab.stoi.keys(), 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9076,\n",
       " 'I, The, It, This, />The, But, And, A, He, There, In, If, />I, They, You, What, She, As, TV, That, DVD, When, American, />This, So, John, Hollywood, One, We, For, Not, However, My, />It, All, No, Even, After, THE, At, Well, To, His, Michael, New, Do, Then, How, Why, While, Also, Of, Some, Just, James, English, David, Mr., />There, With, Now, />In, Robert, />If, British, Man, Richard, Oh, Paul, Although, Oscar, />But, York, George, Jack, OK, On, King, First, God, Peter, Maybe, Unfortunately, Disney, B, Japanese, Joe, Her, Lee, />A, />And, Who, Yes, Is, America, From, />As, Tom, Its, French')"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uppercase_words = [key for key in TEXT.vocab.stoi.keys() if any([c for c in key if c.isupper()])]\n",
    "len(uppercase_words), ', '.join(itertools.islice(uppercase_words, 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(None, {'pos': 0, 'neg': 1})"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LABEL.vocab.stoi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "\n",
    "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data), \n",
    "    batch_size = BATCH_SIZE,\n",
    "    sort_within_batch = True,\n",
    "    device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelWithLSTM(nn.Module):\n",
    "    def __init__(self, embeddings, hidden_dim, output_dim, n_layers, bidirectional, dropout, pad_idx):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding.from_pretrained(embeddings, padding_idx=pad_idx, freeze=False)\n",
    "        \n",
    "        self.rnn = nn.LSTM(embeddings.shape[1], \n",
    "                           hidden_dim, \n",
    "                           num_layers=n_layers, \n",
    "                           bidirectional=bidirectional, \n",
    "                           dropout=dropout)\n",
    "        \n",
    "        num_directions = 2 if bidirectional else 1\n",
    "        self.fc = nn.Linear(hidden_dim * num_directions, output_dim)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "#     def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, \n",
    "#                  bidirectional, dropout, pad_idx):\n",
    "#         super().__init__()\n",
    "        \n",
    "#         self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx = pad_idx)\n",
    "        \n",
    "#         self.rnn = nn.LSTM(embedding_dim, \n",
    "#                            hidden_dim, \n",
    "#                            num_layers=n_layers, \n",
    "#                            bidirectional=bidirectional, \n",
    "#                            dropout=dropout)\n",
    "        \n",
    "#         num_directions = 2 if bidirectional else 1\n",
    "#         self.fc = nn.Linear(hidden_dim * num_directions, output_dim)\n",
    "\n",
    "#         self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, text, text_lengths):\n",
    "        # text = [sentence len, batch size]\n",
    "        \n",
    "        embedded = self.dropout(self.embedding(text))\n",
    "        \n",
    "        # embedded = [sentence len, batch size, emb dim]\n",
    "        \n",
    "        # Pack sequence\n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths)\n",
    "        \n",
    "        _, (hidden, _) = self.rnn(packed_embedded)\n",
    "\n",
    "        # hidden = [num layers * num directions, batch size, hid dim]\n",
    "        \n",
    "        # Concat the final forward (hidden[-2,:,:]) and backward (hidden[-1,:,:]) hidden layers\n",
    "        # and apply dropout\n",
    "        \n",
    "        hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1))\n",
    "                \n",
    "        # hidden = [batch size, hid dim * num directions]\n",
    "\n",
    "        return self.fc(hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ModelWithLSTM(\n",
    "    TEXT.vocab.vectors,\n",
    "    hidden_dim = 256,\n",
    "    output_dim = 1,\n",
    "    n_layers = 2, \n",
    "    bidirectional = True,\n",
    "    dropout = 0.5,\n",
    "    pad_idx = TEXT.vocab.stoi[TEXT.pad_token])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of params in model: 4810857\n"
     ]
    }
   ],
   "source": [
    "print(\"Total number of params in model:\", sum(p.numel() for p in model.parameters() if p.requires_grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [-0.0382, -0.2449,  0.7281,  ..., -0.1459,  0.8278,  0.2706],\n",
       "        ...,\n",
       "        [ 0.2194, -0.4329,  0.1889,  ..., -0.5509,  0.0359,  0.8901],\n",
       "        [-0.1451,  0.4990, -0.4001,  ...,  0.2153,  0.1783,  0.3516],\n",
       "        [-0.2197,  0.5010, -0.0146,  ...,  0.0834, -0.1279,  0.4475]])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.embedding.weight.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_accuracy(preds, y):\n",
    "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
    "    correct = (rounded_preds == y).float()\n",
    "    return correct.sum() / len(correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for batch in iterator:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        text, text_lengths = batch.text\n",
    "        predictions = model(text, text_lengths).squeeze(1)\n",
    "        \n",
    "        loss = criterion(predictions, batch.label)\n",
    "        acc = binary_accuracy(predictions, batch.label)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in iterator:\n",
    "            text, text_lengths = batch.text\n",
    "            predictions = model(text, text_lengths).squeeze(1)\n",
    "            \n",
    "            loss = criterion(predictions, batch.label)\n",
    "            acc = binary_accuracy(predictions, batch.label)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 263,  162,   11,  ...,    0,   11,   11],\n",
      "        [   5, 1584,   34,  ...,    0, 1097,  201],\n",
      "        [1115,    8,  127,  ..., 3445,    0,   16],\n",
      "        ...,\n",
      "        [  16,   82,    6,  ...,    1,    1,    1],\n",
      "        [2801, 5952,  205,  ...,    1,    1,    1],\n",
      "        [   4,    4,    4,  ...,    1,    1,    1]]) tensor(0.6324, grad_fn=<BinaryCrossEntropyWithLogitsBackward>) tensor(0.6250)\n",
      "tensor([[  25,   11,   11,  ...,   54,   11,  162],\n",
      "        [  22,   34, 6973,  ...,   15, 1071, 6384],\n",
      "        [   6,    8,   55,  ...,  279,   64,    3],\n",
      "        ...,\n",
      "        [ 424,   60, 6198,  ...,    1,    1,    1],\n",
      "        [ 825,   38, 3579,  ...,    1,    1,    1],\n",
      "        [   4,    4,    4,  ...,    1,    1,    1]]) tensor(0.6382, grad_fn=<BinaryCrossEntropyWithLogitsBackward>) tensor(0.6250)\n",
      "tensor([[   66,   154,    16,  ..., 16170,    11,   468],\n",
      "        [   14,    32,    22,  ...,   210,   246,   347],\n",
      "        [    0,   414,     9,  ...,  6080,    16,     2],\n",
      "        ...,\n",
      "        [  810,     0,  3372,  ...,     1,     1,     1],\n",
      "        [    4,  3434, 13263,  ...,     1,     1,     1],\n",
      "        [    0,     4,     4,  ...,     1,     1,     1]]) tensor(0.6273, grad_fn=<BinaryCrossEntropyWithLogitsBackward>) tensor(0.6406)\n",
      "tensor([[  111,    11,    66,  ..., 17847,    66,    11],\n",
      "        [  112,   264,  6606,  ...,  2808,     9,    56],\n",
      "        [   18,   228,  6397,  ..., 23895,     5,   309],\n",
      "        ...,\n",
      "        [   16,     2, 10444,  ...,     1,     1,     1],\n",
      "        [    0,   152, 16083,  ...,     1,     1,     1],\n",
      "        [    4,    39,     4,  ...,     1,     1,     1]]) tensor(0.5968, grad_fn=<BinaryCrossEntropyWithLogitsBackward>) tensor(0.6406)\n",
      "tensor([[ 548,   11,  479,  ...,   11,   66,    4],\n",
      "        [ 134, 8002,  373,  ...,  246, 3412,    4],\n",
      "        [ 241,   10,   85,  ...,   16,    9,    4],\n",
      "        ...,\n",
      "        [   5,    2,   39,  ...,    1,    1,    1],\n",
      "        [   0,   24,   39,  ...,    1,    1,    1],\n",
      "        [   4,    4,   39,  ...,    1,    1,    1]]) tensor(0.5889, grad_fn=<BinaryCrossEntropyWithLogitsBackward>) tensor(0.6562)\n",
      "tensor([[  776,    25,    66,  ...,   354,  2043, 22534],\n",
      "        [    4,  1208,    22,  ...,  2750,     4,   155],\n",
      "        [  154,  1274,     9,  ...,     9,   867,   463],\n",
      "        ...,\n",
      "        [   26,     7,    68,  ...,     4,     4,    39],\n",
      "        [    0,  3714,    90,  ...,     1,     1,     1],\n",
      "        [    4,     4,    30,  ...,     1,     1,     1]]) tensor(0.5352, grad_fn=<BinaryCrossEntropyWithLogitsBackward>) tensor(0.7188)\n",
      "tensor([[   65,   148,    66,  ...,    66,   356, 21932],\n",
      "        [    9,   960,    38,  ...,    24,     7,  2703],\n",
      "        [  402,    23,    52,  ...,    19,     2,     4],\n",
      "        ...,\n",
      "        [   12, 17695,     2,  ...,     1,     1,     1],\n",
      "        [    9,    92,   367,  ...,     1,     1,     1],\n",
      "        [    4,   104,     4,  ...,     1,     1,     1]]) tensor(0.5959, grad_fn=<BinaryCrossEntropyWithLogitsBackward>) tensor(0.6875)\n",
      "tensor([[  14, 3940,   11,  ...,    0,  364,   25],\n",
      "        [6862,   83,   84,  ..., 5243, 2510,  338],\n",
      "        [  14,  362,   29,  ...,  410,    2,   19],\n",
      "        ...,\n",
      "        [   9,   60,   56,  ...,  314,    4, 9252],\n",
      "        [1730, 2108,   13,  ...,    1,    1,    1],\n",
      "        [   4,    4,    4,  ...,    1,    1,    1]]) tensor(0.5996, grad_fn=<BinaryCrossEntropyWithLogitsBackward>) tensor(0.6719)\n",
      "tensor([[2378,  658,  313,  ...,   11,  354, 1642],\n",
      "        [   5,   11,   11,  ...,  264,   15,   11],\n",
      "        [ 849,  268,   19,  ...,    5, 7065, 1560],\n",
      "        ...,\n",
      "        [1390,  570,  136,  ...,    1,    1,    1],\n",
      "        [   7,    0,  609,  ...,    1,    1,    1],\n",
      "        [ 314,    0,    4,  ...,    1,    1,    1]]) tensor(0.5578, grad_fn=<BinaryCrossEntropyWithLogitsBackward>) tensor(0.6719)\n",
      "tensor([[    0,     0,    66,  ...,    11,   593,  5546],\n",
      "        [    9, 16626,    24,  ...,    34,    98,   745],\n",
      "        [    5,     3,   319,  ...,    33,   258,  1172],\n",
      "        ...,\n",
      "        [   56,   643,    21,  ...,    22,  3767,    41],\n",
      "        [ 1897,     8,  2033,  ...,     4,   398,     4],\n",
      "        [    4,    58,     4,  ...,     1,     1,     1]]) tensor(0.6366, grad_fn=<BinaryCrossEntropyWithLogitsBackward>) tensor(0.6250)\n",
      "tensor([[  356,   914,    66,  ...,    11,    11,    66],\n",
      "        [    0,  3667,    19,  ...,    56,   201,    22],\n",
      "        [   23,    47,   278,  ...,   309,    16,     9],\n",
      "        ...,\n",
      "        [   42,    59, 15037,  ...,     4,     4,     4],\n",
      "        [   68,  3498,  4302,  ...,     1,     1,     1],\n",
      "        [    4,    39,   549,  ...,     1,     1,     1]]) tensor(0.8287, grad_fn=<BinaryCrossEntropyWithLogitsBackward>) tensor(0.4531)\n",
      "tensor([[ 1520,    66,   313,  ...,    25, 10115,    11],\n",
      "        [ 7866,   339,    16,  ...,   812, 11075,    57],\n",
      "        [  420,     7,   105,  ...,    19,  2224,    29],\n",
      "        ...,\n",
      "        [ 1080,  5020,    83,  ...,     1,     1,     1],\n",
      "        [  483,   497,   131,  ...,     1,     1,     1],\n",
      "        [    4,     4,     4,  ...,     1,     1,     1]]) tensor(0.7264, grad_fn=<BinaryCrossEntropyWithLogitsBackward>) tensor(0.5312)\n",
      "tensor([[   11,   162,    66,  ...,   867, 24081,  7573],\n",
      "        [   73,  4658,    22,  ...,   677,   887,     9],\n",
      "        [  136,     3,     9,  ...,    45,    16,     5],\n",
      "        ...,\n",
      "        [  354,     7,   112,  ...,   570,     4,     4],\n",
      "        [  118,    98,    18,  ...,     1,     1,     1],\n",
      "        [    4,     4,     0,  ...,     1,     1,     1]]) tensor(0.5522, grad_fn=<BinaryCrossEntropyWithLogitsBackward>) tensor(0.6875)\n",
      "tensor([[    0,    25,    25,  ...,   270,    11,    11],\n",
      "        [ 1402,     0,   338,  ...,     5,   119,    19],\n",
      "        [  471,    19,     0,  ...,  1649,  4601,    10],\n",
      "        ...,\n",
      "        [ 1098,     5,    10,  ...,    46,     0,     4],\n",
      "        [   15,  1513,   362,  ..., 16130,     1,     1],\n",
      "        [    4,    39,     4,  ...,     1,     1,     1]]) tensor(0.6130, grad_fn=<BinaryCrossEntropyWithLogitsBackward>) tensor(0.6719)\n",
      "tensor([[11078,    11,   170,  ...,   554,   162, 12855],\n",
      "        [ 5152,    19,    31,  ...,    27,     2,   422],\n",
      "        [   93,    33,    32,  ...,  1248,   134,  6292],\n",
      "        ...,\n",
      "        [   20,    23,    14,  ...,     1,     1,     1],\n",
      "        [   13,    12,  7318,  ...,     1,     1,     1],\n",
      "        [    4,     4,    39,  ...,     1,     1,     1]]) tensor(0.5914, grad_fn=<BinaryCrossEntropyWithLogitsBackward>) tensor(0.6562)\n",
      "tensor([[   66,    11,   148,  ...,    66,  3116,    11],\n",
      "        [    9,   214, 16532,  ...,     9,     3,    62],\n",
      "        [    2,    16,    10,  ...,     2,   405,    29],\n",
      "        ...,\n",
      "        [   61,  1047,   685,  ...,     1,     1,     1],\n",
      "        [ 4870,    39,     4,  ...,     1,     1,     1],\n",
      "        [    4,    39,     1,  ...,     1,     1,     1]]) tensor(0.5329, grad_fn=<BinaryCrossEntropyWithLogitsBackward>) tensor(0.7812)\n",
      "tensor([[ 4832,    11,   548,  ...,    66,  1833,    11],\n",
      "        [   10,    69, 10309,  ...,     9,  2004,   431],\n",
      "        [    2,  1084,    17,  ...,     5,     8,     5],\n",
      "        ...,\n",
      "        [  303,    38,     4,  ...,     1,     1,     1],\n",
      "        [  531,    58,     1,  ...,     1,     1,     1],\n",
      "        [    4,     1,     1,  ...,     1,     1,     1]]) tensor(0.6219, grad_fn=<BinaryCrossEntropyWithLogitsBackward>) tensor(0.5938)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-121-134453acac9a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_iterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mvalid_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_iterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-120-4739dcab8a9f>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, iterator, optimizer, criterion)\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \"\"\"\n\u001b[0;32m--> 118\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     91\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     92\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 5\n",
    "MODEL_PATH = os.path.join(DATA_PATH, \"sentiment_lstm_best.pt\")\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
    "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n",
    "    \n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), MODEL_PATH)\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(MODEL_PATH))\n",
    "\n",
    "test_loss, test_acc = evaluate(model, test_iterator, criterion)\n",
    "\n",
    "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
